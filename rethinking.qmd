---
title: "Rethinking"
format:
  html:
    toc: true
    df-print: paged
---

[Bayesian Analysis - Simon Mak](https://youtu.be/2lgg6OIjd8M)

[![](images/clipboard-3229768047.png)](https://www.elmhurst.edu/blog/thomas-bayes/)

|  |  |
|----|----|
| **observations** | $y$ |
| **parameters** | $\theta$ |
| **prior** | $p(\theta)$ |
| **likelihood** | $p(y \mid \theta)$ |
| **joint** | $p(\theta, y) = p(\theta) \cdot p(y \mid \theta)$ |
| **posterior** | $p(\theta \mid y) = \frac{p(\theta) \cdot p(y \mid \theta)}{\int_{\Theta} p(\theta) \cdot p(y \mid \theta)}$ |

: **Terminology of Bayesian data analysis**

![](images/clipboard-777378815.png)

## The Bayesian paradigm

-   Forward problem

-   Inverse problem

-   Bayes' rule

## Forward problem

```{dot}
digraph G {
    rankdir=LR;
    node [shape=box style=filled fontname="Helvetica" fontcolor=white fontsize=10];

    Observables [label="Output\n(Observables)", fillcolor=green2, color=green4];
    Model [label=<Model>, fillcolor=orangered, color=orangered4];
    Parameters [label="Input\n(Parameters)", fillcolor=royalblue2, color=royalblue4];

    Model -> Observables;
    Parameters -> Model;
}
```

-   **Model**: theoretical descriptions of the relevant processes (DGP)

$$M : \theta \rightarrow y$$

$$y = f(\theta)$$

-   **Input**: model parameters (unobservables)

-   **Output**: data (observables)

## Inverse problem

```{dot}
digraph G {
    rankdir=RL;
    
    node [shape=box style=filled fontname="Helvetica" fontcolor=white fontsize=10];

    Observables [label="Output\n(Observables)", fillcolor=green2, color=green4];
    InverseModel [label=<Model<SUP>-1</SUP>>, fillcolor=orangered, color=orangered4];
    Parameters [label="Input\n(Parameters)", fillcolor=royalblue2, color=royalblue4];

    Observables -> InverseModel;
    InverseModel -> Parameters;
}
```

-   **Inverse Model**:

$$M^{-1} : y \rightarrow \theta$$

$$\theta = f^{-1}(y)$$

-   **Input**: model parameters (unobservables)

-   **Output**: data (observables)

## Setup environment

```{r}
#| output: false
library(tidyverse)
library(easystats)
library(rstan)
library(cmdstanr)
library(rethinking)
library(brms)
library(posterior)
library(bayesplot)
library(loo)
library(shinystan)
library(bayestestR)
library(tidybayes)
library(tidybayes.rethinking)
library(broom)
library(broom.mixed)
library(ggformula)
library(ggdist)
library(mosaic)
library(modelsummary)
library(marginaleffects)
library(emmeans)
library(modelbased)

options(mc.cores = parallel::detectCores(), brms.backend = "cmdstanr")

rstan_options(auto_write = TRUE)
rstan_options(threads_per_chain = 1)

check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
register_knitr_engine(override = FALSE)

theme_set(theme_bw())

set.rseed(666)
```

## Scientific question and statistical estimand

XXX

## Specify scientific model

```{dot}
digraph BetaBinomialDGP {
    rankdir=TB;
    node [shape=ellipse, style=filled, fillcolor=lightgray];

    # Nodes with simple names but detailed labels
    alpha_beta [label="α, β (Hyperparameters)" fillcolor=lightblue];
    theta [label="θ (Latent Proportion)" fillcolor=lightgreen];
    y [label="y (Observed Count)" fillcolor=lightcoral];
    N [label="N (Total Trials)" fillcolor=lightgray];

    # Edges with descriptive labels
    alpha_beta -> theta [label="Beta(α, β)"];
    theta -> y [label="Binomial(n, θ)"];
    N -> y;

    # Graph Label
    label="Beta-Binomial Data Generating Process (DGP)";
    fontsize=14;
}
```

## Specify statistical model

```{r}
f <- alist(
  y ~ dbern(pi),
  pi ~ dbeta(1, 1)
)
```

## Condition statistical model

```{r}
fit <- quap(f, data = list(y = c(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1)))
```

## Summarize posterior

```{r}
summary(fit)
```

```{r}
fit %>% tidy_draws() %>% median_qi()
```

```{r}
fit %>% tidy_draws() %>% ggplot(aes(x = pi)) + stat_halfeye()
```

\

## Print environment

```{r}
sessioninfo::session_info()
```
